# llama-chat Configuration File
# This file is optimized for Dell Optiplex 3070 running i3-9100T CPU with 8GB RAM
# Format: KEY=VALUE (no spaces around =)
# Lines starting with # are comments

# Server Configuration
LLAMACPP_PORT=8120
LLAMACPP_HOST=127.0.0.1
FLASK_PORT=3333
FLASK_HOST=127.0.0.1

# Performance Settings
CONTEXT_SIZE=4096
GPU_LAYERS=0
THREADS=4
BATCH_SIZE=512

# Model Management
DEFAULT_MODEL="qwen2.5-0.5b-instruct-q4_0.gguf"
MODEL_SWITCH_TIMEOUT=60
AUTO_RESTART_ON_CRASH=true

# Health Monitoring
AUTO_RESTART_ON_CRASH=true
HEALTH_CHECK_INTERVAL=15

# Log rotation
LOG_MAX_SIZE=100M
LOG_ROTATE_COUNT=5

# Model download configuration
MODEL_DOWNLOAD_TIMEOUT=3600
MODEL_DOWNLOAD_RETRIES=3
MODEL_VERIFY_CHECKSUM=false

