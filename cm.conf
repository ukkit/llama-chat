# llama-chat Configuration File
# This file contains configuration options for llama-chat and llama.cpp server
# Format: KEY=VALUE (no spaces around =)
# Lines starting with # are comments

# ============================================================================
# INSTALLATION SETTINGS
# ============================================================================

# Installation directory
INSTALL_DIR=$HOME/llama-chat

# ============================================================================
# FLASK APPLICATION SETTINGS
# ============================================================================

# Flask web server configuration
FLASK_HOST=127.0.0.1
FLASK_PORT=3333
FLASK_DEBUG=false

# ============================================================================
# LLAMA.CPP SERVER SETTINGS
# ============================================================================

# Basic server configuration
LLAMACPP_HOST=127.0.0.1
LLAMACPP_PORT=8120
MODELS_DIR=$INSTALL_DIR/models

# Model settings
DEFAULT_MODEL=
CONTEXT_SIZE=4096
GPU_LAYERS=0
THREADS=4
BATCH_SIZE=512

# ============================================================================
# ADVANCED LLAMA.CPP SERVER OPTIONS
# Uncomment and modify as needed
# ============================================================================

# Processing and Performance
# LLAMA_ARG_N_PARALLEL=1
# LLAMA_ARG_CONT_BATCHING=false
# LLAMA_ARG_N_THREADS_BATCH=4
# LLAMA_ARG_N_UBATCH=512
# LLAMA_ARG_N_KEEP=-1

# Memory Management
# LLAMA_ARG_MLOCK=false
# LLAMA_ARG_NO_MMAP=false
# LLAMA_ARG_NUMA=false

# Model Loading
# LLAMA_ARG_N_CTX=4096
# LLAMA_ARG_N_BATCH=512
# LLAMA_ARG_N_GPU_LAYERS=0
# LLAMA_ARG_MAIN_GPU=0
# LLAMA_ARG_TENSOR_SPLIT=

# RoPE (Rotary Position Embedding) Settings
# LLAMA_ARG_ROPE_FREQ_BASE=0.0
# LLAMA_ARG_ROPE_FREQ_SCALE=0.0
# LLAMA_ARG_ROPE_SCALING_TYPE=

# Advanced RoPE/YaRN Settings
# LLAMA_ARG_YARN_EXT_FACTOR=-1.0
# LLAMA_ARG_YARN_ATTN_FACTOR=1.0
# LLAMA_ARG_YARN_BETA_FAST=32.0
# LLAMA_ARG_YARN_BETA_SLOW=1.0
# LLAMA_ARG_YARN_ORIG_CTX=0

# Features
# LLAMA_ARG_EMBEDDING=false
# LLAMA_ARG_RERANKING=false
# LLAMA_ARG_MULTIMODAL=false

# ============================================================================
# SECURITY SETTINGS
# ============================================================================

# API Authentication (uncomment to enable)
# LLAMA_ARG_API_KEY=
# LLAMA_ARG_API_KEY_FILE=

# CORS and Network Security
# LLAMA_ARG_CORS=false
# LLAMA_ARG_TIMEOUT=600

# ============================================================================
# SAMPLING AND GENERATION SETTINGS
# ============================================================================

# Default sampling parameters
# LLAMA_ARG_TEMP=0.8
# LLAMA_ARG_TOP_K=40
# LLAMA_ARG_TOP_P=0.95
# LLAMA_ARG_MIN_P=0.05
# LLAMA_ARG_TFS_Z=1.0
# LLAMA_ARG_TYPICAL_P=1.0
# LLAMA_ARG_REPEAT_PENALTY=1.1
# LLAMA_ARG_REPEAT_LAST_N=64
# LLAMA_ARG_PENALIZE_NL=true
# LLAMA_ARG_PRESENCE_PENALTY=0.0
# LLAMA_ARG_FREQUENCY_PENALTY=0.0
# LLAMA_ARG_MIROSTAT=0
# LLAMA_ARG_MIROSTAT_TAU=5.0
# LLAMA_ARG_MIROSTAT_ETA=0.1

# ============================================================================
# SYSTEM PROMPT AND CHAT FORMAT
# ============================================================================

# Chat and prompt settings
# LLAMA_ARG_SYSTEM_PROMPT_FILE=
# LLAMA_ARG_CHAT_TEMPLATE=
# LLAMA_ARG_ESCAPE=false

# ============================================================================
# SERVER BEHAVIOR
# ============================================================================

# Request handling
# LLAMA_ARG_SLOTS=true
# LLAMA_ARG_METRICS=false
# LLAMA_ARG_SLOT_SAVE_PATH=
# LLAMA_ARG_CHAT_SAVE_PATH=

# Logging and debugging
# LLAMA_ARG_LOG_FORMAT=
# LLAMA_ARG_LOG_DISABLE=false
# LLAMA_ARG_VERBOSE=false

# ============================================================================
# HARDWARE-SPECIFIC OPTIMIZATIONS
# ============================================================================

# CUDA (NVIDIA GPU) Settings
# LLAMA_ARG_CUDA_NONBLOCKING=false
# LLAMA_ARG_FLASH_ATTN=false

# Metal (Apple GPU) Settings
# LLAMA_ARG_METAL=true

# OpenCL Settings
# LLAMA_ARG_OPENCL=false
# LLAMA_ARG_OPENCL_PLATFORM_ID=0

# BLAS Settings
# LLAMA_ARG_BLAS=false

# ============================================================================
# LOGGING CONFIGURATION
# ============================================================================

# Log file locations
LOG_DIR=$INSTALL_DIR/logs
LLAMACPP_LOG_FILE=$LOG_DIR/llamacpp.log
FLASK_LOG_FILE=$LOG_DIR/flask.log

# Log rotation
LOG_MAX_SIZE=100M
LOG_ROTATE_COUNT=5

# ============================================================================
# DEVELOPMENT AND TESTING
# ============================================================================

# Development settings
DEV_MODE=false
DEV_RELOAD=false

# Testing settings
TEST_MODE=false
TEST_PORT=3001

# ============================================================================
# MODEL DOWNLOAD SETTINGS
# ============================================================================

# Model download configuration
MODEL_DOWNLOAD_TIMEOUT=3600
MODEL_DOWNLOAD_RETRIES=3
MODEL_VERIFY_CHECKSUM=false

# ============================================================================
# PRESET CONFIGURATIONS
# Choose one by uncommenting the relevant section
# ============================================================================

# ------------------ CPU-ONLY CONFIGURATION (4GB RAM) ------------------
# CONTEXT_SIZE=2048
# GPU_LAYERS=0
# BATCH_SIZE=256
# LLAMA_ARG_N_PARALLEL=1
# LLAMA_ARG_MLOCK=false
# LLAMA_ARG_NO_MMAP=true

# ------------------ SMALL GPU CONFIGURATION (4-6GB VRAM) ------------------
# CONTEXT_SIZE=4096
# GPU_LAYERS=20
# BATCH_SIZE=512
# LLAMA_ARG_N_PARALLEL=1
# LLAMA_ARG_MAIN_GPU=0

# ------------------ MEDIUM GPU CONFIGURATION (8-12GB VRAM) ------------------
# CONTEXT_SIZE=8192
# GPU_LAYERS=32
# BATCH_SIZE=1024
# LLAMA_ARG_N_PARALLEL=2
# LLAMA_ARG_FLASH_ATTN=true
# LLAMA_ARG_CONT_BATCHING=true

# ------------------ HIGH-END GPU CONFIGURATION (16GB+ VRAM) ------------------
# CONTEXT_SIZE=16384
# GPU_LAYERS=-1
# BATCH_SIZE=2048
# LLAMA_ARG_N_PARALLEL=4
# LLAMA_ARG_FLASH_ATTN=true
# LLAMA_ARG_CONT_BATCHING=true
# LLAMA_ARG_N_UBATCH=1024

# ------------------ MULTI-GPU CONFIGURATION ------------------
# CONTEXT_SIZE=8192
# GPU_LAYERS=-1
# BATCH_SIZE=1024
# LLAMA_ARG_N_PARALLEL=4
# LLAMA_ARG_MAIN_GPU=0
# LLAMA_ARG_TENSOR_SPLIT=0.6,0.4
# LLAMA_ARG_CONT_BATCHING=true

# ------------------ PRODUCTION SERVER CONFIGURATION ------------------
# LLAMACPP_HOST=0.0.0.0
# FLASK_HOST=0.0.0.0
# CONTEXT_SIZE=4096
# GPU_LAYERS=32
# LLAMA_ARG_API_KEY=your-secure-api-key-here
# LLAMA_ARG_CORS=false
# LLAMA_ARG_TIMEOUT=300
# LLAMA_ARG_SLOTS=false
# LLAMA_ARG_METRICS=true
# LLAMA_ARG_LOG_FORMAT=json

# ------------------ DEVELOPMENT CONFIGURATION ------------------
# FLASK_DEBUG=true
# DEV_MODE=true
# DEV_RELOAD=true
# CONTEXT_SIZE=2048
# GPU_LAYERS=0
# LLAMA_ARG_VERBOSE=true
# LLAMA_ARG_SLOTS=true
# LLAMA_ARG_METRICS=true