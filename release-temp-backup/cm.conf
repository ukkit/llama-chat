# llama-chat Configuration File
# This file is optimized for Dell Optiplex 3070 running i3-9100T CPU with 8GB RAM
# Format: KEY=VALUE (no spaces around =)
# Lines starting with # are comments

# ============================================================================
# INSTALLATION SETTINGS
# ============================================================================

# Installation directory
INSTALL_DIR=$HOME/llama-chat
MODELS_DIR=$INSTALL_DIR/models

# Server Configuration
LLAMACPP_PORT=8120
LLAMACPP_HOST=127.0.0.1
FLASK_PORT=3333
FLASK_HOST=127.0.0.1

# Performance Settings
CONTEXT_SIZE=4096
GPU_LAYERS=0
THREADS=4
BATCH_SIZE=512

# Model Management
DEFAULT_MODEL="qwen2.5-0.5b-instruct-q4_0.gguf"
MODEL_SWITCH_TIMEOUT=60
AUTO_RESTART_ON_CRASH=true

# Health Monitoring
AUTO_RESTART_ON_CRASH=true
HEALTH_CHECK_INTERVAL=15

# Log file locations
LOG_DIR=$INSTALL_DIR/logs
LLAMACPP_LOG_FILE=$LOG_DIR/llamacpp.log
FLASK_LOG_FILE=$LOG_DIR/flask.log

# Log rotation
LOG_MAX_SIZE=100M
LOG_ROTATE_COUNT=5

# Performance Optimizations
USE_MMAP=true
USE_MLOCK=false

# Model download configuration
MODEL_DOWNLOAD_TIMEOUT=3600
MODEL_DOWNLOAD_RETRIES=3
MODEL_VERIFY_CHECKSUM=false
